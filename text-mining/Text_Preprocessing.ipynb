{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ad1315",
   "metadata": {},
   "source": [
    "***Sentence Tokenize***\n",
    "\n",
    "Tokenizing is the process of breaking a large set of texts into smaller meaningful chunks such as sentences, words, phrases. NLTK library provides sent_tokenize for sentence level tokenizing, which uses a pre-trained model PunktSentenceTokenize, to determine punctuation and characters marking the end of sentence for European language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6483c3a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T10:39:11.419349302Z",
     "start_time": "2023-12-08T10:39:10.240343090Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kazi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e39a4f84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T10:39:12.974552882Z",
     "start_time": "2023-12-08T10:39:12.960533153Z"
    }
   },
   "outputs": [],
   "source": [
    "text = 'Statistics skills, and programming skills are equally important for analytics. Statistics skills, and domain knowledge are important for analytics. I like reading books and travelling.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00cf0637",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T10:39:49.090577887Z",
     "start_time": "2023-12-08T10:39:49.046528551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statistics skills, and programming skills are equally important for analytics.', 'Statistics skills, and domain knowledge are important for analytics.', 'I like reading books and travelling.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print(sent_tokenize_list)\n",
    "# for sentence in sent_tokenize_list:\n",
    "#     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc4b501",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T10:40:04.741388908Z",
     "start_time": "2023-12-08T10:40:04.719262954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Hola.', 'Esta es una frase espanola.']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are total 17 european languages that NLTK support for sentence tokenize\n",
    "# Let's try loading a spanish model\n",
    "import nltk.data\n",
    "\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "spanish_tokenizer.tokenize('Hola. Esta es una frase espanola.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e2bad",
   "metadata": {},
   "source": [
    "***Word Tokenize***\n",
    "\n",
    "word_tokenize is a wrapper function that calls tokenize by the TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "234008c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T10:40:10.807924368Z",
     "start_time": "2023-12-08T10:40:10.793828897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statistics', 'skills', ',', 'and', 'programming', 'skills', 'are', 'equally', 'important', 'for', 'analytics', '.', 'Statistics', 'skills', ',', 'and', 'domain', 'knowledge', 'are', 'important', 'for', 'analytics', '.', 'I', 'like', 'reading', 'books', 'and', 'travelling', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69ffea35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T10:40:11.335563397Z",
     "start_time": "2023-12-08T10:40:11.329206019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statistics', 'skills', ',', 'and', 'programming', 'skills', 'are', 'equally', 'important', 'for', 'analytics.', 'Statistics', 'skills', ',', 'and', 'domain', 'knowledge', 'are', 'important', 'for', 'analytics.', 'I', 'like', 'reading', 'books', 'and', 'travelling', '.']\n"
     ]
    }
   ],
   "source": [
    "# Another equivalent call method\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0fd0aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Statistics', 'skills', ',', 'and', 'programming', 'skills', 'are', 'equally', 'important', 'for', 'analytics', '.', 'Statistics', 'skills', ',', 'and', 'domain', 'knowledge', 'are', 'important', 'for', 'analytics', '.', 'I', 'like', 'reading', 'books', 'and', 'travelling', '.']\n"
     ]
    }
   ],
   "source": [
    "# Except the TreebankWordTokenizer, there are other alternative word tokenizers, such as PunktWordTokenizer and WordPunktTokenizer\n",
    "# PunktTokenizer splits on punctuation, but keeps it with the word\n",
    "# from nltk.tokenize import PunktWordTokenizer\n",
    "# punkt_word_tokenizer = PunktWordTokenizer()\n",
    "# print punkt_word_tokenizer.tokenize(text) \n",
    "\n",
    "# WordPunctTokenizer splits all punctuations into separate tokens\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print(word_punct_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample English sentence with punctuations\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def remove_punctuations(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    punt_removed = [w for w in words if w.lower() not in string.punctuation]\n",
    "    return \" \".join(punt_removed)\n",
    "\n",
    "\n",
    "print(remove_punctuations('This is a sample English sentence, with punctuations!'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T10:47:17.871887792Z",
     "start_time": "2023-12-08T10:47:17.828262182Z"
    }
   },
   "id": "1c24cd8b2103a11e"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample English sentence\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def remove_stopwords(text, lang='english'):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lang_stopwords = stopwords.words(lang)\n",
    "    stopwords_removed = [w for w in words if w.lower() not in lang_stopwords]\n",
    "    return \" \".join(stopwords_removed)\n",
    "print (remove_stopwords('This is a sample English sentence'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T10:49:22.496696745Z",
     "start_time": "2023-12-08T10:49:22.440037589Z"
    }
   },
   "id": "c241c0c6144446e2"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed whitespace:  This is a sample English sentence, with whitespace and numbers 1234!\n"
     ]
    }
   ],
   "source": [
    "# Function to remove whitespace\n",
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "text = 'This is a sample English sentence, \\n with whitespace and numbers 1234!'\n",
    "print('Removed whitespace: ', remove_whitespace(text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T10:50:27.711413205Z",
     "start_time": "2023-12-08T10:50:27.664224407Z"
    }
   },
   "id": "8e9dd8948e831318"
  },
  {
   "cell_type": "markdown",
   "id": "5227df03",
   "metadata": {},
   "source": [
    "***PoS tagging***\n",
    "\n",
    "The default pos tagger model using in NLTK is maxent_treebanck_pos_tagger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9079127",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T10:52:05.360060756Z",
     "start_time": "2023-12-08T10:52:02.971470738Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kazi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/kazi/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/kazi/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('English', 'JJ'), ('sentence', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import chunk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "tagged_sent = nltk.pos_tag(nltk.word_tokenize('This is a sample English sentence'))\n",
    "print(tagged_sent)\n",
    "\n",
    "tree = chunk.ne_chunk(tagged_sent)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04474d79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T10:52:07.052132285Z",
     "start_time": "2023-12-08T10:52:07.026066971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/kazi/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# To get help about tags\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3844ad75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('English', 'JJ'), ('sentence', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "\n",
    "PT = PerceptronTagger()\n",
    "print(PT.tag('This is a sample English sentence'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9651ba",
   "metadata": {},
   "source": [
    "***Remove stopwords***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89967beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kazi/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample English sentence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(text, lang='english'):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lang_stopwords = stopwords.words(lang)\n",
    "    stopwords_removed = [w for w in words if w.lower() not in lang_stopwords]\n",
    "    return \" \".join(stopwords_removed)\n",
    "\n",
    "\n",
    "print(remove_stopwords('This is a sample English sentence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de334cd",
   "metadata": {},
   "source": [
    "***Remove punctuations***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4cc7802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample English sentence with punctuations\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "# Function to remove punctuations\n",
    "def remove_punctuations(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    punt_removed = [w for w in words if w.lower() not in string.punctuation]\n",
    "    return \" \".join(punt_removed)\n",
    "\n",
    "\n",
    "print(remove_punctuations('This is a sample English sentence, with punctuations!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f31794",
   "metadata": {},
   "source": [
    "***Remove whitespace & numbers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f38e2de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  This \tis a     sample  English   sentence, \n",
      " with whitespace and numbers 1234!\n",
      "Removed whitespace:  This is a sample English sentence, with whitespace and numbers 1234!\n",
      "Removed numbers:  This \tis a     sample  English   sentence, \n",
      " with whitespace and numbers !\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Function to remove whitespace\n",
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "# Function to remove numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "text = 'This \tis a     sample  English   sentence, \\n with whitespace and numbers 1234!'\n",
    "print('Original Text: ', text)\n",
    "print('Removed whitespace: ', remove_whitespace(text))\n",
    "print('Removed numbers: ', remove_numbers(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924e75d6",
   "metadata": {},
   "source": [
    "***Stemming***\n",
    "\n",
    "\n",
    "It is the process of transforming to the root word i.e., it uses an algorithm that removes common word endings for English words, such as “ly”, “es”, “ed” and “s”. For example, assuming for an analysis you may want to consider “carefully”, “cared”, “cares”, “caringly” as “care” instead of separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02b9d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f509d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  caring cares cared caringly carefully\n",
      "Porter:  care care care caringli care\n",
      "Lancaster:  car car car car car\n",
      "Snowball:  care care care care care\n"
     ]
    }
   ],
   "source": [
    "# Function to apply stemming to a list of words\n",
    "def words_stemmer(words, type=\"PorterStemmer\", lang=\"english\"):\n",
    "    supported_stemmers = [\"PorterStemmer\", \"LancasterStemmer\", \"SnowballStemmer\"]\n",
    "    if type is False or type not in supported_stemmers:\n",
    "        return words\n",
    "    else:\n",
    "        stem_words = []\n",
    "        if type == \"PorterStemmer\":\n",
    "            stemmer = PorterStemmer()\n",
    "            for word in words:\n",
    "                stem_words.append(stemmer.stem(word))\n",
    "        if type == \"LancasterStemmer\":\n",
    "            stemmer = LancasterStemmer()\n",
    "            for word in words:\n",
    "                stem_words.append(stemmer.stem(word))\n",
    "        if type == \"SnowballStemmer\":\n",
    "            stemmer = SnowballStemmer(lang)\n",
    "            for word in words:\n",
    "                stem_words.append(stemmer.stem(word))\n",
    "        return \" \".join(stem_words)\n",
    "\n",
    "\n",
    "words = 'caring cares cared caringly carefully'\n",
    "\n",
    "print(\"Original: \", words)\n",
    "print(\"Porter: \", words_stemmer(nltk.word_tokenize(words), \"PorterStemmer\"))\n",
    "print(\"Lancaster: \", words_stemmer(nltk.word_tokenize(words), \"LancasterStemmer\"))\n",
    "print(\"Snowball: \", words_stemmer(nltk.word_tokenize(words), \"SnowballStemmer\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4142c73",
   "metadata": {},
   "source": [
    "**Lemmatizer**\n",
    "\n",
    "It is the process of transforming to the dictionary base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7b235d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T12:25:11.076567550Z",
     "start_time": "2023-12-08T12:25:10.292332795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized:  care care care caringly carefully\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Function to apply lemmatization to a list of words\n",
    "def words_lemmatizer(text, encoding=\"utf8\"):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemma_words = []\n",
    "    wl = WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        pos = find_pos(word)\n",
    "        lemma_words.append(wl.lemmatize(word, pos).encode(encoding).decode(encoding))\n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "\n",
    "# Function to find part of speech tag for a word\n",
    "def find_pos(word):\n",
    "    # Part of Speech constants\n",
    "    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    # You can learn more about these at http://wordnet.princeton.edu/wordnet/man/wndb.5WN.html#sect3\n",
    "    # You can learn more about all the Penn Treebank tags at https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "    # Adjective tags - 'JJ', 'JJR', 'JJS'\n",
    "    if pos.lower()[0] == 'j':\n",
    "        return 'a'\n",
    "    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "    elif pos.lower()[0] == 'r':\n",
    "        return 'r'\n",
    "    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "    elif pos.lower()[0] == 'v':\n",
    "        return 'v'\n",
    "    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "\n",
    "word = 'caring cares cared caringly carefully'\n",
    "print(\"Lemmatized: \", words_lemmatizer(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58eb7b41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T12:25:11.162506475Z",
     "start_time": "2023-12-08T12:25:11.081618595Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition:  benefit\n",
      "Example:  ['for your own good', \"what's the good of worrying?\"]\n",
      "synonyms: \n",
      " {'salutary', 'in_force', 'well', 'estimable', 'soundly', 'secure', 'thoroughly', 'practiced', 'good', 'honest', 'full', 'near', 'proficient', 'honorable', 'unspoiled', 'goodness', 'dependable', 'trade_good', 'dear', 'in_effect', 'sound', 'upright', 'skilful', 'adept', 'ripe', 'undecomposed', 'unspoilt', 'respectable', 'beneficial', 'skillful', 'just', 'commodity', 'effective', 'serious', 'safe', 'right', 'expert'}\n",
      "antonyms: \n",
      " {'evilness', 'ill', 'evil', 'badness', 'bad'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syns = wordnet.synsets(\"good\")\n",
    "print(\"Definition: \", syns[0].definition())\n",
    "print(\"Example: \", syns[0].examples())\n",
    "\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "# Print  synonums and antonyms (having opposite meaning words)\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(\"synonyms: \\n\", set(synonyms))\n",
    "print(\"antonyms: \\n\", set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf6b56",
   "metadata": {},
   "source": [
    "***N-grams***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42a24248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T12:25:13.478920075Z",
     "start_time": "2023-12-08T12:25:13.471086380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  ['This', 'is', 'a', 'sample', 'English', 'sentence']\n",
      "2-gram:  ['This is', 'is a', 'a sample', 'sample English', 'English sentence']\n",
      "3-gram:  ['This is a', 'is a sample', 'a sample English', 'sample English sentence']\n",
      "4-gram:  ['This is a sample', 'is a sample English', 'a sample English sentence']\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Function to extract n-grams from text\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = ngrams(nltk.word_tokenize(text), n)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "\n",
    "text = 'This is a sample English sentence'\n",
    "\n",
    "print(\"1-gram: \", get_ngrams(text, 1))\n",
    "print(\"2-gram: \", get_ngrams(text, 2))\n",
    "print(\"3-gram: \", get_ngrams(text, 3))\n",
    "print(\"4-gram: \", get_ngrams(text, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dfa28d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T12:25:14.623521616Z",
     "start_time": "2023-12-08T12:25:14.603882254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  dict_keys(['Statistics skills', 'skills and', 'and programming', 'programming skills', 'skills are', 'are equally', 'equally important', 'important for', 'for analytics', 'analytics Statistics', 'and domain', 'domain knowledge', 'knowledge are', 'are important'])\n",
      "\n",
      "Frequency:  dict_values([2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "text = 'Statistics skills, and programming skills are equally important for analytics. Statistics skills, and domain knowledge are important for analytics'\n",
    "\n",
    "# remove punctuations\n",
    "text = remove_punctuations(text)\n",
    "\n",
    "# Extracting bigrams\n",
    "result = get_ngrams(text, 2)\n",
    "\n",
    "# Counting bigrams\n",
    "result_count = Counter(result)\n",
    "\n",
    "print(\"Words: \", result_count.keys())  # Bigrams\n",
    "print(\"\\nFrequency: \", result_count.values())  # Bigram frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3c88c7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T12:25:15.149182770Z",
     "start_time": "2023-12-08T12:25:14.959058721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                      frequency\nStatistics skills             2\nskills and                    2\nand programming               1\nprogramming skills            1\nskills are                    1\nare equally                   1\nequally important             1\nimportant for                 2\nfor analytics                 2\nanalytics Statistics          1\nand domain                    1\ndomain knowledge              1\nknowledge are                 1\nare important                 1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Statistics skills</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>skills and</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>and programming</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>programming skills</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>skills are</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>are equally</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>equally important</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>important for</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>for analytics</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>analytics Statistics</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>and domain</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>domain knowledge</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>knowledge are</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>are important</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting to the result to a data frame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(result_count, orient='index')\n",
    "df = df.rename(columns={'index': 'words', 0: 'frequency'})  # Renaming index and column name\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7561f04",
   "metadata": {},
   "source": [
    "***Bag of Words (BoW)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39b71a10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T12:29:36.715824739Z",
     "start_time": "2023-12-08T12:29:36.672507456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'docs': ['Statistics skills, and domain knowledge are important for analytics.', 'I like reading books and travelling.', 'Statistics skills, and programming skills are equally important for analytics.'], 'ColNames': <map object at 0x7f71e18e9b70>}\n"
     ]
    },
    {
     "data": {
      "text/plain": "             Doc_2.txt  Doc_3.txt  Doc_1.txt\nanalytics            1          0          1\nand                  1          1          1\nare                  1          0          1\nbooks                0          1          0\ndomain               1          0          0\nequally              0          0          1\nfor                  1          0          1\nimportant            1          0          1\nknowledge            1          0          0\nlike                 0          1          0\nprogramming          0          0          1\nreading              0          1          0\nskills               1          0          2\nstatistics           1          0          1\ntravelling           0          1          0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Doc_2.txt</th>\n      <th>Doc_3.txt</th>\n      <th>Doc_1.txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>analytics</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>are</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>books</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>domain</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>equally</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>for</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>important</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>knowledge</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>like</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>programming</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>reading</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>skills</th>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>statistics</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>travelling</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Create a dictionary with key as file names and values as text for all files in a given folder\n",
    "def CorpusFromDir(dir_path):\n",
    "    result = dict(docs=[open(os.path.join(dir_path, f)).read() for f in os.listdir(dir_path)],\n",
    "                  ColNames=map(lambda x: x, os.listdir(dir_path)))\n",
    "    return result\n",
    "\n",
    "\n",
    "docs = CorpusFromDir('Data/text_files/')\n",
    "print(docs)\n",
    "\n",
    "# Initialize\n",
    "vectorizer = CountVectorizer()\n",
    "doc_vec = vectorizer.fit_transform(docs.get('docs'))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(doc_vec.toarray().transpose(), index=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Change column headers to be file names\n",
    "df.columns = docs.get('ColNames')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0e61c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T12:29:36.923113137Z",
     "start_time": "2023-12-08T12:29:36.880905988Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05836401",
   "metadata": {},
   "source": [
    "***TF-IDF***\n",
    "\n",
    "In the area of information retrieval TF-IDF is a good statistical measure to reflect the relevance of term to the document in a collection of documents or corpus. Let’s break TF_IDF and apply example to understand it better.\n",
    "\n",
    "TF (term) = (Number of times term appears in a document)/(Total number of terms in the document) IDF (term) = log⁡( (Total number of documents)/(Number of documents with a given term in it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c36c2a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T12:29:37.280389387Z",
     "start_time": "2023-12-08T12:29:37.272641150Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7634eba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T12:29:37.489086028Z",
     "start_time": "2023-12-08T12:29:37.460525041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             Doc_2.txt  Doc_3.txt  Doc_1.txt\nanalytics     0.315269   0.000000   0.276703\nand           0.244835   0.283217   0.214884\nare           0.315269   0.000000   0.276703\nbooks         0.000000   0.479528   0.000000\ndomain        0.414541   0.000000   0.000000\nequally       0.000000   0.000000   0.363831\nfor           0.315269   0.000000   0.276703\nimportant     0.315269   0.000000   0.276703\nknowledge     0.414541   0.000000   0.000000\nlike          0.000000   0.479528   0.000000\nprogramming   0.000000   0.000000   0.363831\nreading       0.000000   0.479528   0.000000\nskills        0.315269   0.000000   0.553405\nstatistics    0.315269   0.000000   0.276703\ntravelling    0.000000   0.479528   0.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Doc_2.txt</th>\n      <th>Doc_3.txt</th>\n      <th>Doc_1.txt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>analytics</th>\n      <td>0.315269</td>\n      <td>0.000000</td>\n      <td>0.276703</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>0.244835</td>\n      <td>0.283217</td>\n      <td>0.214884</td>\n    </tr>\n    <tr>\n      <th>are</th>\n      <td>0.315269</td>\n      <td>0.000000</td>\n      <td>0.276703</td>\n    </tr>\n    <tr>\n      <th>books</th>\n      <td>0.000000</td>\n      <td>0.479528</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>domain</th>\n      <td>0.414541</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>equally</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.363831</td>\n    </tr>\n    <tr>\n      <th>for</th>\n      <td>0.315269</td>\n      <td>0.000000</td>\n      <td>0.276703</td>\n    </tr>\n    <tr>\n      <th>important</th>\n      <td>0.315269</td>\n      <td>0.000000</td>\n      <td>0.276703</td>\n    </tr>\n    <tr>\n      <th>knowledge</th>\n      <td>0.414541</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>like</th>\n      <td>0.000000</td>\n      <td>0.479528</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>programming</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.363831</td>\n    </tr>\n    <tr>\n      <th>reading</th>\n      <td>0.000000</td>\n      <td>0.479528</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>skills</th>\n      <td>0.315269</td>\n      <td>0.000000</td>\n      <td>0.553405</td>\n    </tr>\n    <tr>\n      <th>statistics</th>\n      <td>0.315269</td>\n      <td>0.000000</td>\n      <td>0.276703</td>\n    </tr>\n    <tr>\n      <th>travelling</th>\n      <td>0.000000</td>\n      <td>0.479528</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Create a dictionary with key as file names and values as text for all files in a given folder\n",
    "def CorpusFromDir(dir_path):\n",
    "    result = dict(docs=[open(os.path.join(dir_path, f)).read() for f in os.listdir(dir_path)],\n",
    "                  ColNames=map(lambda x: x, os.listdir(dir_path)))\n",
    "    return result\n",
    "\n",
    "\n",
    "docs = CorpusFromDir('Data/text_files/')\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_vec = vectorizer.fit_transform(docs.get('docs'))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(doc_vec.toarray().transpose(), index=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Change column headers to be file names\n",
    "df.columns = docs.get('ColNames')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb5355",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059021c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd8c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
