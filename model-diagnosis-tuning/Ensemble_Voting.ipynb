{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d386c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# set seed for reproducability\n",
    "np.random.seed(2017)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# currently its available as part of mlxtend and not sklearn\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da6e4400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Diabetes.csv\")\n",
    "\n",
    "X = df.iloc[:,:8]     # independent variables\n",
    "y = df['class']     # dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffddd9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2017)\n",
    "\n",
    "LR = LogisticRegression(random_state=2017)\n",
    "RF = RandomForestClassifier(n_estimators = 100, random_state=2017)\n",
    "SVM = SVC(random_state=0, probability=True)\n",
    "KNC = KNeighborsClassifier()\n",
    "DTC = DecisionTreeClassifier()\n",
    "ABC = AdaBoostClassifier(n_estimators = 100)\n",
    "BC = BaggingClassifier(n_estimators = 100)\n",
    "GBC = GradientBoostingClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e8710d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.76 (+/- 0.04) [Logistic Regression]\n",
      "Test Accuracy: 0.81 \n",
      "Train CV Accuracy: 0.74 (+/- 0.03) [Random Forest]\n",
      "Test Accuracy: 1.00 \n",
      "Train CV Accuracy: 0.74 (+/- 0.04) [Support Vector Machine]\n",
      "Test Accuracy: 0.81 \n",
      "Train CV Accuracy: 0.70 (+/- 0.05) [KNeighbors]\n",
      "Test Accuracy: 0.84 \n",
      "Train CV Accuracy: 0.69 (+/- 0.02) [Decision Tree]\n",
      "Test Accuracy: 1.00 \n",
      "Train CV Accuracy: 0.73 (+/- 0.04) [Ada Boost]\n",
      "Test Accuracy: 0.83 \n",
      "Train CV Accuracy: 0.74 (+/- 0.03) [Bagging]\n",
      "Test Accuracy: 1.00 \n",
      "Train CV Accuracy: 0.75 (+/- 0.03) [Gradient Boosting]\n",
      "Test Accuracy: 0.92 \n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "print('5-fold cross validation:\\n')\n",
    "for clf, label in zip([LR, RF, SVM, KNC, DTC, ABC, BC, GBC], \n",
    "                      ['Logistic Regression', \n",
    "                       'Random Forest', \n",
    "                       'Support Vector Machine',\n",
    "                       'KNeighbors',\n",
    "                       'Decision Tree',\n",
    "                       'Ada Boost',\n",
    "                       'Bagging',\n",
    "                       'Gradient Boosting']):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X, y)    \n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982926c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.76 (+/- 0.04) [Logistic Regression]\n",
      "Test Accuracy: 0.81 \n",
      "Train CV Accuracy: 0.74 (+/- 0.03) [Random Forest]\n",
      "Test Accuracy: 1.00 \n",
      "Train CV Accuracy: 0.74 (+/- 0.04) [Gradient Boosting]\n",
      "Test Accuracy: 0.92 \n"
     ]
    }
   ],
   "source": [
    "clfs = []\n",
    "print('5-fold cross validation:\\n')\n",
    "for clf, label in zip([LR, RF, GBC], \n",
    "                      ['Logistic Regression', \n",
    "                       'Random Forest', \n",
    "                       'Gradient Boosting']):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X, y)    \n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0757e5",
   "metadata": {},
   "source": [
    "***Hard voting vs Soft voting***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba7ef87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold cross validation:\n",
      "\n",
      "Train CV Accuracy: 0.75 (+/- 0.02) [Ensemble Hard Voting]\n",
      "Test Accuracy: 0.93 \n",
      "Train CV Accuracy: 0.76 (+/- 0.02) [Ensemble Soft Voting]\n",
      "Test Accuracy: 0.94 \n"
     ]
    }
   ],
   "source": [
    "# ### Ensemble Voting\n",
    "clfs = []\n",
    "print('5-fold cross validation:\\n')\n",
    "\n",
    "ECH = EnsembleVoteClassifier(clfs=[LR, RF, GBC], voting='hard')\n",
    "ECS = EnsembleVoteClassifier(clfs=[LR, RF, GBC], voting='soft', weights=[1,1,1])\n",
    "\n",
    "for clf, label in zip([ECH, ECS], \n",
    "                      ['Ensemble Hard Voting', \n",
    "                       'Ensemble Soft Voting']):\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n",
    "    md = clf.fit(X, y)    \n",
    "    clfs.append(md)\n",
    "    print(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_test), y_test)))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a49a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
